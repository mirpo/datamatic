version: 1.0

steps:
  - name: download_the_doc
    type: shell
    cmd: curl https://raw.githubusercontent.com/mirpo/chopdoc/refs/heads/main/tests/pg_essay.txt -O
    outputFilename: pg_essay.txt

  - name: chopdoc_by_sentence
    type: shell
    cmd: chopdoc --input pg_essay.txt --output pg_essay.jsonl --method sentence --size 10 --overlap 2
    outputFilename: pg_essay.jsonl

  - name: questions_per_chunk
    model: ollama:deepseek-r1:1.5b
    maxResults: chopdoc_by_sentence.$length # for each chunk generate 3 questions
    prompt: |
      For the following text chunk, generate exactly **3 questions**, each based on a different level of cognitive thinking. For each question, you must also provide direct text evidence from the chunk that supports or relates to the question.

      **Text Chunk:**
      {{.chopdoc_by_sentence.chunk}}

      ---
      **Question Generation Guidelines:**

      1. **Understanding Level**
        - Focus on basic facts and concepts.
        - Use "What", "Who", "When", or "Where" style questions.
        - Goal: Ensure clear identification and comprehension.

      2. **Analysis Level**
        - Explore relationships, causes, and implications.
        - Use "How" or "Why" questions.
        - Goal: Examine reasoning and connections within the content.

      3. **Synthesis Level**
        - Apply or extend ideas creatively, or propose solutions.
        - Use "What if", "How might", or "Suggest a solution" style questions.
        - Goal: Encourage integration, problem-solving, or imaginative application.
      ---
      **Output Format:**
      Provide your response as a JSON object with a single top-level key `questions`, which contains an array of exactly three question objects. Each question object must have two keys: `question` (string) and `textEvidence` (string). The `textEvidence` should be a concise sentence or phrase directly from the provided chunk.

      Do not include any conversational text or explanations outside of the JSON.

    jsonSchema:
      type: object
      properties:
        questions:
          type: array
          minItems: 3
          maxItems: 3
          items:
            type: object
            properties:
              question:
                type: string
              textEvidence:
                type: string
            required:
              - question
              - textEvidence
      required:
        - questions

  - name: flatten_question_chunk
    type: shell
    cmd: |
      cat ./questions_per_chunk.jsonl | jq -c '.values.".chopdoc_by_sentence.chunk".value as $chunk | .response.questions[] | { question: .question, chunk: $chunk, textEvidence: .textEvidence}' > flatten_question_chunk.jsonl
    outputFilename: flatten_question_chunk.jsonl

  - name: validate_questions
    model: ollama:deepseek-r1:1.5b
    maxResults: flatten_question_chunk.$length # for each question chunk validate
    prompt: |
      prompt: |
      Evaluate the provided question based *solely* on its alignment with the accompanying text chunk and the effectiveness of the supplied text evidence. Your assessment should be rigorous, ensuring accuracy and relevance to the given content.

      ---
      **Question:**
      {{.flatten_question_chunk.question}}

      **Text Chunk:**
      {{.flatten_question_chunk.chunk}}

      **Text Evidence (if provided):**
      {{.flatten_question_chunk.textEvidence}}
      ---

      **Evaluation Criteria:**
      1. **Relevance** – Does the question directly reflect the main idea or key details of the chunk? Is it pertinent to the content?
      2. **Accuracy** – Is the question factually correct and answerable based *only* on the information present in the chunk?
      3. **Depth** – Does the question effectively align with a higher cognitive level (e.g., Understanding, Analysis, or Synthesis) if implied by its structure, even if the original level wasn't explicitly stated here?
      4. **Evidence Match** – If text evidence is provided, does it directly and strongly support or relate to the question being asked?

      ---
      **Rating Scale (1–10):**
      - **1–3 (Poor):** Irrelevant, factually incorrect, or unsupported by the text. The evidence, if present, does not match.
      - **4–6 (Fair):** Some relevance or accuracy, but may lack clarity, depth, or strong supporting evidence.
      - **7–9 (Good):** Generally accurate and relevant, with reasonable depth and supportive evidence.
      - **10 (Excellent):** Highly accurate, deeply relevant to the chunk, insightful, and strongly and directly backed by the provided text evidence.

      ---
      **Output Format:**
      Provide your response as a JSON object containing a single key `rating`, with an integer value between 1 and 10, representing your overall assessment.
      Do not include any additional text or explanations.

    jsonSchema:
      type: object
      properties:
        rating:
          type: integer
          minimum: 1
          maximum: 10
      required:
        - rating

  - name: filter_with_high_rating
    type: shell
    cmd: |
      cat ./validate_questions.jsonl | jq -c '. | select(.response.rating >= 7)' > filter_with_high_rating.jsonl
    outputFilename: filter_with_high_rating.jsonl
